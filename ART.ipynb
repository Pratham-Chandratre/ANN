{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01f2145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6824d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ART:\n",
    "    def __init__(self, input_size, rho, alpha):\n",
    "        self.W = np.ones((1, input_size))\n",
    "        self.rho = rho\n",
    "        self.alpha = alpha\n",
    " \n",
    "    def reset(self):\n",
    "        self.W = np.ones((1, input_size))\n",
    " \n",
    "    def train(self, x):\n",
    "        while True:\n",
    "            y = self.predict(x)\n",
    "            if y is not None:\n",
    "                self.update(x)\n",
    "                return\n",
    "            else:\n",
    "                self.reset()\n",
    "\n",
    "    def predict(self, x):\n",
    "        y = x.dot(self.W.T)\n",
    "        if y >= self.rho:\n",
    "            return y\n",
    "        else:\n",
    "             return None\n",
    " \n",
    "    def update(self, x):\n",
    "        self.W = self.alpha * x + (1 - self.alpha) * self.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e73cf9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8637 0.8853]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "   input_size = 2\n",
    "   rho = 0.9\n",
    "   alpha = 0.1\n",
    "   network = ART(input_size, rho, alpha)\n",
    "   x1 = np.array([0.7, 0.3])\n",
    "   x2 = np.array([0.2, 0.8])\n",
    "   x3 = np.array([0.6, 0.6])\n",
    "   network.train(x1)\n",
    "   network.train(x2)\n",
    "   network.train(x3)\n",
    "   print(network.W)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba62914",
   "metadata": {},
   "source": [
    "This code snippet implements a simple Adaptive Resonance Theory (ART) model for unsupervised learning. ART is a family of neural network algorithms designed to stabilize learning and prevent \"catastrophic forgetting.\" It is often used in clustering tasks where the goal is to identify and group similar patterns without predefined labels. Here's a detailed explanation of the code and the theory behind it.\n",
    "\n",
    "Initialization: The class constructor initializes the weight vector W, vigilance parameter rho, and learning rate alpha.\n",
    "Weights: The W vector starts with all ones and represents the network's internal state. The weights will be updated during training to represent learned patterns.\n",
    "Vigilance Parameter (rho): This parameter controls the network's sensitivity to differences between input patterns and existing clusters. A higher rho value indicates stricter clustering criteria.\n",
    "Learning Rate (alpha): Determines the rate at which weights are updated during training. A smaller alpha leads to slower adaptation, while a larger alpha results in faster learning.\n",
    "    \n",
    "Reset: The reset method resets the weights to their initial state (all ones). This is used when the current pattern doesn't meet the clustering criteria.\n",
    "Training: The train method takes an input pattern x and aims to find a cluster that resonates with it. If a matching cluster is found, the weights are updated. If not, the weights are reset, implying that the current pattern does not fit the existing cluster, and a new one is formed.\n",
    "    \n",
    "Dot Product: The predict method computes the dot product between the input x and the transpose of the weights W. This measures the similarity between the input pattern and the existing cluster.\n",
    "Vigilance Check: The output y is compared to the vigilance parameter rho. If y is greater than or equal to rho, it indicates that the input pattern matches the current cluster. Otherwise, it does not meet the threshold for clustering.\n",
    "    \n",
    "Learning Rule: The update method adjusts the weights based on the learning rate alpha. The new weights are a combination of the input pattern and the current weights, with the proportion determined by alpha.\n",
    "Purpose of Update: This step allows the network to adapt to new patterns while retaining information from previous clusters.\n",
    "\n",
    "Parameters: The main program defines the input size (2), vigilance parameter (rho = 0.9), and learning rate (alpha = 0.1).\n",
    "Training Data: It creates three sample inputs (x1, x2, x3) and trains the ART network with them.\n",
    "Training Process: Each input is passed to the train method, which either updates the weights or resets them based on whether the pattern meets the vigilance criteria.\n",
    "Final Weights: The final output of the program prints the weights after training, showing how the network has adapted to the input patterns.\n",
    "    \n",
    "Conclusion\n",
    "This code implements a simple ART model, which is used for unsupervised learning and clustering. The vigilance parameter rho controls the network's sensitivity to differences in patterns, while the learning rate alpha governs the pace of adaptation. The code trains the ART network with a set of input patterns and adjusts the weights accordingly. This approach demonstrates how ART can be used to cluster similar patterns and adapt to new information while avoiding catastrophic forgetting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f3bf31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
