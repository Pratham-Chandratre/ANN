{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ba9974f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.3040 - accuracy: 0.9156\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 5s 6ms/step - loss: 0.1346 - accuracy: 0.9603\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.0937 - accuracy: 0.9728\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.0719 - accuracy: 0.9787\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.0567 - accuracy: 0.9832\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 5s 6ms/step - loss: 0.0448 - accuracy: 0.9866\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 5s 6ms/step - loss: 0.0370 - accuracy: 0.9889\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 5s 6ms/step - loss: 0.0304 - accuracy: 0.9911\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 5s 6ms/step - loss: 0.0246 - accuracy: 0.9932\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 5s 6ms/step - loss: 0.0196 - accuracy: 0.9949\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0765 - accuracy: 0.9782\n",
      "Test Loss: 0.07652948051691055\n",
      "Test Accuracy: 0.9782000184059143\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=10, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe4daec",
   "metadata": {},
   "source": [
    "TensorFlow: A popular machine learning framework developed by Google for building and deploying neural networks.\n",
    "Keras: The high-level API within TensorFlow used to create neural networks in a user-friendly way.\n",
    "MNIST Dataset: A benchmark dataset for handwritten digit classification, widely used in machine learning.\n",
    "Sequential Model: A simple linear model that is built by stacking layers one after the other.\n",
    "Dense and Flatten Layers: Common layers used in neural networks. Flatten transforms multidimensional data into a 1D vector, while Dense is a fully connected layer.\n",
    "Adam Optimizer: An optimization algorithm that uses adaptive learning rates and momentum.\n",
    "\n",
    "Load Data: The mnist.load_data() function loads the dataset into training and testing sets. Each set consists of image data (X) and labels (y).\n",
    "Normalization: Dividing by 255 scales the pixel values from [0, 255] to [0, 1]. This normalization helps with faster convergence during training and reduces sensitivity to varying input scales.\n",
    "    \n",
    "Flatten Layer: The first layer in the model, converting 2D images (28x28) into a 1D vector of 784 elements. This prepares the data for the fully connected layers.\n",
    "Dense Layer (128 units): A fully connected layer with 128 units and ReLU (Rectified Linear Unit) activation. ReLU introduces non-linearity, helping the model learn complex patterns.\n",
    "Dense Layer (10 units): The output layer with 10 units, corresponding to the 10 digits (0-9). The softmax activation function ensures the output is a probability distribution, summing to 1. This is useful for multi-class classification, as it indicates the predicted probability for each class.\n",
    "\n",
    "    \n",
    "Optimizer (Adam): Adam is a commonly used optimizer in deep learning, known for its adaptive learning rates and fast convergence.\n",
    "Loss Function (Sparse Categorical Cross-Entropy): This loss function is used for multi-class classification tasks with sparse labels (labels are integers, not one-hot encoded). It calculates the cross-entropy loss, which measures the difference between the predicted probabilities and the true class.\n",
    "Metrics (Accuracy): Accuracy is a common metric for classification tasks, indicating the proportion of correct predictions.\n",
    "    \n",
    "Training: The fit method trains the model on the training dataset.\n",
    "Batch Size (64): The number of samples processed in each training step. Smaller batch sizes can lead to faster training but more noise, while larger batch sizes are smoother but require more memory.\n",
    "Epochs (10): The number of times the entire training dataset is passed through the model during training.\n",
    "Verbose: Controls the amount of output during training. A value of 1 displays a progress bar and training metrics.\n",
    "    \n",
    "Evaluation: The evaluate method measures the model's performance on the test dataset, providing the loss and accuracy.\n",
    "Loss: Indicates how well the model's predictions align with the actual labels. Lower loss typically means better performance.\n",
    "Accuracy: A measure of the proportion of correct predictions. Higher accuracy indicates better classification performance.\n",
    "\n",
    "Conclusion\n",
    "This code demonstrates a simple feedforward neural network for the MNIST dataset. The model includes a Flatten layer to convert 2D images to a 1D vector, followed by a fully connected (Dense) layer with ReLU activation and an output layer with softmax activation. The code normalizes the input data, compiles the model with Adam optimizer, and trains the model with a batch size of 64 over 10 epochs. The evaluation step assesses the model's performance on the test set, providing a measure of accuracy and loss. Overall, this code snippet serves as a good starting point for understanding basic neural network concepts and their application to digit classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4d816d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
